{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install boruta\n"
      ],
      "metadata": {
        "id": "BdifxB_Bwpec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sklearn-genetic"
      ],
      "metadata": {
        "id": "a4EAv1VUw2IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V94rcKFdtE08"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import datetimea\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import warnings\n",
        "import os\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.feature_selection import f_classif, f_regression, mutual_info_classif, mutual_info_regression, SelectKBest, RFE\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from boruta import BorutaPy\n",
        "from sklearn.utils import check_X_y\n",
        "from scipy import sparse\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "SAVE_LEARNING_CURVE = True  # Toggle to True if you want to plot learning curves\n",
        "SAVE_SHAP_PNG = True\n",
        "\n",
        "# === Feature Logging ===\n",
        "def log_feature_importance(features, scores, method):\n",
        "    df = pd.DataFrame({\"Feature\": features, \"Importance\": scores})\n",
        "    filename = f\"feature_importance_{method}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"üìÑ Feature importances saved to {filename}\")\n",
        "\n",
        "# === Evaluate model performance ===\n",
        "def evaluate_model(task_type, model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    if task_type == 'r':\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}, r2\n",
        "    else:\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        return {'Accuracy': acc, 'Report': report}, acc\n",
        "\n",
        "# === Plot learning curve ===\n",
        "def plot_learning_curve(estimator, X, y, task_type, title=\"Learning Curve\", filename=None):\n",
        "    if not SAVE_LEARNING_CURVE:\n",
        "        return\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=5, scoring='r2' if task_type == 'r' else 'accuracy',\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1)\n",
        "\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid(True)\n",
        "    if filename:\n",
        "        plt.savefig(filename)\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# === Encode categoricals ===\n",
        "def encode_categoricals(df):\n",
        "    df_encoded = df.copy()\n",
        "    encoder = OrdinalEncoder()\n",
        "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "    if not cat_cols.empty:\n",
        "        df_encoded[cat_cols] = encoder.fit_transform(df_encoded[cat_cols].astype(str))\n",
        "    return df_encoded\n",
        "\n",
        "# === Feature Selection ===\n",
        "def apply_feature_selection(method, X, y, task_type, mi_threshold=0.01, p_threshold=0.05):\n",
        "    if method == 'f_test':\n",
        "        score_func = f_regression if task_type == 'r' else f_classif\n",
        "        selector = SelectKBest(score_func=score_func, k='all')\n",
        "        selector.fit(X, y)\n",
        "        pvalues = selector.pvalues_\n",
        "        selected_cols = X.columns[np.where(pvalues < p_threshold)[0]].tolist()\n",
        "        log_feature_importance(X.columns, pvalues, method)\n",
        "\n",
        "    elif method == 'mutual_info':\n",
        "        score_func = mutual_info_regression if task_type == 'r' else mutual_info_classif\n",
        "        selector = SelectKBest(score_func=score_func, k='all')\n",
        "        selector.fit(X, y)\n",
        "        scores = selector.scores_\n",
        "        selected_cols = X.columns[np.where(scores > mi_threshold)[0]].tolist()\n",
        "        log_feature_importance(X.columns, scores, method)\n",
        "\n",
        "    elif method == 'boruta':\n",
        "        model = RandomForestRegressor(n_jobs=-1, random_state=42) if task_type == 'r' else RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "        X_array, y_array = check_X_y(X, y)\n",
        "        X_array = pd.DataFrame(X_array, columns=X.columns)\n",
        "        boruta_selector = BorutaPy(model, n_estimators='auto', verbose=0, random_state=42)\n",
        "        boruta_selector.fit(X_array.values, y_array)\n",
        "        selected_cols = X.columns[boruta_selector.support_].tolist()\n",
        "\n",
        "    elif method == 'rfe':\n",
        "        estimator = LinearRegression() if task_type == 'r' else LogisticRegression(solver='liblinear')\n",
        "        selector = RFE(estimator, n_features_to_select=int(X.shape[1] * 0.5))\n",
        "        selector.fit(X, y)\n",
        "        selected_cols = X.columns[selector.support_].tolist()\n",
        "        log_feature_importance(X.columns, selector.ranking_, method)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported feature selection method\")\n",
        "\n",
        "    print(f\"‚úÖ Selected Features by {method}: {selected_cols}\")\n",
        "    return X[selected_cols]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === PSI Calculation ===\n",
        "def calculate_psi(expected, actual, buckets=10):\n",
        "    expected = np.array(expected)\n",
        "    actual = np.array(actual)\n",
        "    if np.std(expected) == 0 or np.std(actual) == 0:\n",
        "        return 0\n",
        "    breakpoints = np.linspace(0, 1, buckets + 1)\n",
        "    scale = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x) + 1e-9)\n",
        "    expected_scaled = scale(expected)\n",
        "    actual_scaled = scale(actual)\n",
        "    expected_bins = np.histogram(expected_scaled, bins=breakpoints)[0] / len(expected)\n",
        "    actual_bins = np.histogram(actual_scaled, bins=breakpoints)[0] / len(actual)\n",
        "    psi_values = []\n",
        "    for e, a in zip(expected_bins, actual_bins):\n",
        "        if e == 0: e = 1e-4\n",
        "        if a == 0: a = 1e-4\n",
        "        psi_values.append((e - a) * np.log(e / a))\n",
        "    return np.sum(psi_values)\n",
        "\n",
        "# === SHAP Explanation ===\n",
        "def explain_model_with_shap(model, X_sample):\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    explainer.check_additivity = False\n",
        "    shap_values = explainer.shap_values(X_sample)\n",
        "    shap.summary_plot(shap_values, X_sample)\n",
        "\n",
        "# === Conditional SMOTE ===\n",
        "def apply_smote_if_needed(X_train, y_train, task_type):\n",
        "    if task_type != 'c':\n",
        "        print(\"‚ÑπÔ∏è SMOTE not applied: Task is regression.\")\n",
        "        return X_train, y_train\n",
        "\n",
        "    class_counts = Counter(y_train)\n",
        "    total = sum(class_counts.values())\n",
        "    imbalance = any((count / total) < 0.2 for count in class_counts.values())\n",
        "\n",
        "    if imbalance:\n",
        "        print(f\"‚ö†Ô∏è Imbalanced classes detected: {class_counts}\")\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "        print(f\"‚úÖ SMOTE applied: {Counter(y_res)}\")\n",
        "        return X_res, y_res\n",
        "    else:\n",
        "        print(\"‚úÖ No significant class imbalance detected. Skipping SMOTE.\")\n",
        "        return X_train, y_train\n",
        "\n",
        "# === Main Pipeline ===\n",
        "def self_healing_ai_framework():\n",
        "    print(\"\\U0001F4D8 Self-Healing AI Framework with Conditional Healing\")\n",
        "    file_path = input(\"Enter CSV file path or name: \").strip()\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"‚úÖ Dataset loaded successfully.\")\n",
        "    print(\"\\nüßæ Dataset Columns:\", list(df.columns))\n",
        "\n",
        "    target_column = input(\"Enter Target Column: \").strip()\n",
        "    task_type = input(\"Task Type (c=Classification, r=Regression): \").strip().lower()\n",
        "    if task_type not in ['c', 'r']:\n",
        "        print(\"Invalid task type! Choose 'c' or 'r'. Exiting.\")\n",
        "        return\n",
        "\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "    print(\"\\n‚ö†Ô∏è Dropping rows with missing values (NaNs)...\")\n",
        "    initial_shape = X.shape\n",
        "    X = X.dropna(axis=0)\n",
        "    y = y.loc[X.index]\n",
        "    print(f\"Dropped {initial_shape[0] - X.shape[0]} rows due to missing values.\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,\n",
        "                                                        stratify=y if task_type == 'c' else None)\n",
        "\n",
        "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
        "        ('num', StandardScaler(), num_cols)\n",
        "    ])\n",
        "\n",
        "    preprocessor.fit(X_train)\n",
        "    X_train_trans = preprocessor.transform(X_train)\n",
        "    X_test_trans = preprocessor.transform(X_test)\n",
        "\n",
        "    X_train_arr = X_train_trans.toarray() if hasattr(X_train_trans, \"toarray\") else X_train_trans\n",
        "    X_test_arr = X_test_trans.toarray() if hasattr(X_test_trans, \"toarray\") else X_test_trans\n",
        "\n",
        "    X_train_arr, y_train = apply_smote_if_needed(X_train_arr, y_train, task_type)\n",
        "\n",
        "    print(\"\\nüîß Training Base Model (Random Forest)\")\n",
        "    base_model = RandomForestRegressor(random_state=42) if task_type == 'r' else RandomForestClassifier(random_state=42)\n",
        "    base_model.fit(X_train_arr, y_train)\n",
        "    base_perf, base_score = evaluate_model(task_type, base_model, X_test_arr, y_test)\n",
        "    print(f\"üìä Base Model Performance: {base_perf}\")\n",
        "    plot_learning_curve(base_model, X_train_arr, y_train, task_type, title=\"Base Model Learning Curve\")\n",
        "\n",
        "    print(\"\\nüîç SHAP Feature Importance for Base Model\")\n",
        "    X_train_df = pd.DataFrame(X_train_arr, columns=preprocessor.get_feature_names_out())\n",
        "    X_train_sampled = X_train_df.sample(min(100, len(X_train_df)), random_state=42)\n",
        "    explain_model_with_shap(base_model, X_train_sampled)\n",
        "\n",
        "    print(\"\\nüìà Drift Detection (PSI Scores):\")\n",
        "    drift_detected = False\n",
        "    X_train_enc = encode_categoricals(X_train)\n",
        "    X_test_enc = encode_categoricals(X_test)\n",
        "\n",
        "    for col in X_train_enc.columns:\n",
        "        try:\n",
        "            psi = calculate_psi(X_train_enc[col], X_test_enc[col])\n",
        "            if psi > 0.2:\n",
        "                print(f\"‚ö†Ô∏è Drift detected in column '{col}' | PSI = {psi:.4f}\")\n",
        "                drift_detected = True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error calculating PSI for column {col}: {e}\")\n",
        "\n",
        "    if not drift_detected:\n",
        "        print(\"‚úÖ No significant drift detected. Healing phase skipped.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüõ†Ô∏è Healing Phase Triggered...\")\n",
        "    fs_methods = ['f_test', 'mutual_info', 'boruta', 'rfe']\n",
        "\n",
        "    models = {\n",
        "        'RandomForest': RandomForestRegressor(random_state=42) if task_type == 'r' else RandomForestClassifier(random_state=42),\n",
        "        'DecisionTree': DecisionTreeRegressor(random_state=42) if task_type == 'r' else DecisionTreeClassifier(random_state=42),\n",
        "        'SVM': SVR() if task_type == 'r' else SVC(probability=True),\n",
        "        'KNN': KNeighborsRegressor() if task_type == 'r' else KNeighborsClassifier()\n",
        "    }\n",
        "\n",
        "    best_model, best_score, best_combo = None, -npa.inf if task_type == 'r' else 0, (None, None)\n",
        "\n",
        "    for fs in fs_methods:\n",
        "        print(f\"\\nüîç Feature Selection: {fs}\")\n",
        "        try:\n",
        "            X_enc = encode_categoricals(X)\n",
        "            X_fs = apply_feature_selection(fs, X_enc, y, task_type)\n",
        "            X_train_fs = X_train[X_fs.columns]\n",
        "            X_test_fs = X_test[X_fs.columns]\n",
        "\n",
        "            cat_fs = X_train_fs.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "            num_fs = X_train_fs.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "            preproc_fs = ColumnTransformer([\n",
        "                ('cat', OneHotEncoder(handle_unknown='ignore'), cat_fs),\n",
        "                ('num', StandardScaler(), num_fs)\n",
        "            ])\n",
        "\n",
        "            preproc_fs.fit(X_train_fs)\n",
        "            X_train_fs_tr = preproc_fs.transform(X_train_fs)\n",
        "            X_test_fs_tr = preproc_fs.transform(X_test_fs)\n",
        "\n",
        "            X_train_fs_arr = X_train_fs_tr.toarray() if hasattr(X_train_fs_tr, \"toarray\") else X_train_fs_tr\n",
        "            X_test_fs_arr = X_test_fs_tr.toarray() if hasattr(X_test_fs_tr, \"toarray\") else X_test_fs_tr\n",
        "\n",
        "            X_train_model, y_train_model = apply_smote_if_needed(X_train_fs_arr, y_train, task_type)\n",
        "\n",
        "            for name, model in models.items():\n",
        "                print(f\"Training {name} with {fs}...\")\n",
        "                model.fit(X_train_model, y_train_model)\n",
        "                perf, score = evaluate_model(task_type, model, X_test_fs_arr, y_test)\n",
        "                print(f\"Performance: {perf}\")\n",
        "                plot_learning_curve(model, X_train_model, y_train_model, task_type,\n",
        "                                    title=f\"{name} + {fs} Learning Curve\")\n",
        "                if (task_type == 'r' and score > best_score) or (task_type == 'c' and score > best_score):\n",
        "                    best_score, best_model, best_combo = score, model, (name, fs)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå FS={fs} failed: {e}\")\n",
        "\n",
        "    if best_model:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"best_model_{best_combo[0]}_{best_combo[1]}_{timestamp}.pkl\"\n",
        "        joblib.dump(best_model, filename)\n",
        "        print(f\"\\n‚úÖ Best Model Saved: {filename} | Model: {best_combo[0]} | FS: {best_combo[1]} | Score: {best_score:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No suitable model found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    self_healing_ai_framework()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNluhiIAxRwS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}